{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fdf99ca-e30d-4ed4-bda7-de27a46bf3ac",
   "metadata": {},
   "source": [
    " ----------------------------------------**Model_Conversion.ipynb**-------------------------------------------------------------------\n",
    "\n",
    "Part of the Startup-Demos Project, under the MIT License<br>\n",
    "See https://github.com/qualcomm/Startup-Demos/blob/main/LICENSE.txt for license information.<br>\n",
    "Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.<br>\n",
    "SPDX-License-Identifier: MIT License\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0ac1",
   "metadata": {},
   "source": [
    "ResNet50 Emotion Architecture Extractor\n",
    "---------------------------------------\n",
    "- Downloads the `run_webcam.ipynb` from Hugging Face (ElenaRyumina/face_emotion_recognition),\n",
    "- Extracts the model architecture cell containing `class Bottleneck` and `def ResNet50`,\n",
    "- Writes a clean Python module `resnet50_emo.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96da6b1-e831-44d8-9e6c-b4ee69b3b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ResNet50 emotion model architecture from a Hugging Face notebook into a standalone Python module.\n",
    "\n",
    "import os\n",
    "import textwrap\n",
    "import nbformat\n",
    "import torch  # noqa: F401\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# 1) Download the notebook from Hugging Face (version-aware & cached)\n",
    "nb_path = hf_hub_download(\n",
    "    repo_id=\"ElenaRyumina/face_emotion_recognition\",\n",
    "    filename=\"run_webcam.ipynb\",\n",
    "    repo_type=\"model\",\n",
    "    # Optional: pin a specific revision for reproducibility:\n",
    "    # revision=\"f4944b0...\"\n",
    ")\n",
    "\n",
    "# 2) Parse the notebook and locate the model-architecture code cell\n",
    "nb = nbformat.read(nb_path, as_version=4)\n",
    "\n",
    "model_cell_src = None\n",
    "for cell in nb.cells:\n",
    "    if cell.get(\"cell_type\") == \"code\":\n",
    "        src = cell.get(\"source\", \"\")\n",
    "        if (\"class Bottleneck\" in src) and (\"def ResNet50\" in src):\n",
    "            model_cell_src = src\n",
    "            break\n",
    "\n",
    "if model_cell_src is None:\n",
    "    raise RuntimeError(\"Model architecture cell not found; the notebook may have changed.\")\n",
    "\n",
    "# 3) Clean cell code: strip notebook magics (e.g., %, !) if any\n",
    "model_code_clean = \"\\n\".join(\n",
    "    line for line in model_cell_src.splitlines()\n",
    "    if not line.strip().startswith(\"%\") and not line.strip().startswith(\"!\")\n",
    ")\n",
    "\n",
    "# 4) Write the architecture to a Python module\n",
    "module_path = \"resnet50_emo.py\"\n",
    "header = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    import math\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    \"\"\"\n",
    ").strip() + \"\\n\\n\"\n",
    "\n",
    "with open(module_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(header)\n",
    "    f.write(model_code_clean)\n",
    "\n",
    "print(f\"[INFO] Saved model architecture to: {os.path.abspath(module_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4d5a8-aa5f-406f-9c23-bec05a33e521",
   "metadata": {},
   "source": [
    "ResNet50 Emotion Model Loader\n",
    "-----------------------------\n",
    "- Imports the extracted ResNet50 architecture, builds the model, loads a checkpoint.\n",
    "- Performs a quick sanity check in eval mode.\n",
    "- Download and load FER_static_ResNet50_AffectNet.pt from Hugging Face using huggingface_hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379959f-166b-4717-aeb3-391ef14f977e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 AffectNet emotion model weights into the standalone PyTorch architecture\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from collections import OrderedDict\n",
    "from resnet50_emo import ResNet50\n",
    "\n",
    "# Download into local cache; version-aware\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"ElenaRyumina/face_emotion_recognition\",\n",
    "    filename=\"FER_static_ResNet50_AffectNet.pt\",\n",
    "    repo_type=\"model\",\n",
    "    # revision=\"main\"  # optionally pin a specific commit/tag for reproducibility\n",
    ")\n",
    "\n",
    "# Build and load\n",
    "model = ResNet50(num_classes=7, channels=3)\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "if any(k.startswith(\"module.\") for k in state_dict.keys()):\n",
    "    state_dict = OrderedDict((k.replace(\"module.\", \"\"), v) for k, v in state_dict.items())\n",
    "\n",
    "missing_unexp = model.load_state_dict(state_dict, strict=True)\n",
    "print(\"[INFO] load_state_dict (missing, unexpected):\", missing_unexp)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71f1cd-2306-470d-a363-29a47c0f55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell converts the PyTorch emotion recognition model into an ONNX file for use in other frameworks.\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Output path: current working directory\n",
    "onnx_path = os.path.join(os.getcwd(), \"resnet50_emotion.onnx\")\n",
    "\n",
    "# Dummy input (NCHW float32, 224x224)\n",
    "dummy = torch.randn(1, 3, 224, 224, dtype=torch.float32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                       # your nn.Module\n",
    "    dummy,                       # example input\n",
    "    onnx_path,                   # output file\n",
    "    input_names=[\"image\"],       # match AI Hub input_specs\n",
    "    output_names=[\"logits\"],     # convenient name\n",
    "    opset_version=13,            # good default for pad/conv\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\"image\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
    "    training=torch.onnx.TrainingMode.EVAL,\n",
    "    export_params=True\n",
    ")\n",
    "\n",
    "print(f\"[INFO] ONNX model saved to: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9f5d6-0500-4640-924e-80dcdc523f30",
   "metadata": {},
   "source": [
    "\n",
    "Compile ONNX with AI Hub\n",
    "-----------------------------------------------------\n",
    "Submits a compile job to Dragonwing RB3 Gen 2 Vision Kit using the ONNX\n",
    "model found in the current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3e79d-de42-46b2-a268-d51360907b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit and run an AI Hub compile job to build the ONNX ResNet50 emotion model for the RB3 Gen 2 target\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import qai_hub as hub\n",
    "\n",
    "# Device\n",
    "DEVICE = hub.Device(\"Dragonwing RB3 Gen 2 Vision Kit\")\n",
    "\n",
    "# ONNX path from current working directory\n",
    "onnx_path = Path(os.getcwd()) / \"resnet50_emotion.onnx\"  # ensure this file exists\n",
    "\n",
    "# Create and submit the compile job with input specs and runtime/output option\n",
    "compile_job_static = hub.submit_compile_job(\n",
    "    model=str(onnx_path),\n",
    "    device=DEVICE,\n",
    "    name=\"EMO_AffectNet_ONNX_model\",\n",
    "    input_specs={\"image\": ((1, 3, 224, 224), \"float32\")},\n",
    "    options=\"--target_runtime onnx --output_names logits\",\n",
    ")\n",
    "# Block until the compile job finishes and retrieve the compiled target model artifact\n",
    "compile_job_static.wait()\n",
    "static_model = compile_job_static.get_target_model()\n",
    "print(\"[INFO] Compile job completed. Target model path:\", static_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766d048-2896-4967-962d-58c8225202eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Calibartion Data\n",
    "-----------------------\n",
    "- Use Option A when you have a real image dataset in your working directory\n",
    "- Option B when you want synthetic calibration samples (no external data needed).\n",
    "\n",
    "__Note__ \n",
    "- For best quantization accuracy, Option A (real image dataset) is recommended because real samples better represent the activation. \n",
    "- Option B (synthetic samples) is useful for quick bring-up or when real data is unavailable, but may lead to slightly lower accuracy compared to real calibration data.\n",
    "- Choose any one option(optionA or optionB) for running the Calibartion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d092217-5a0d-4f83-b74b-59d547752e2c",
   "metadata": {},
   "source": [
    "Option A: Image Dataset Calibration Samples\n",
    "----------------------------\n",
    "- Manual Download the Image Dataset\n",
    "- Go to [FER2013 Kaggle page](https://www.kaggle.com/datasets/msambare/fer2013).\n",
    "- Click Download and unzip the dataset into your project folder (e.g., .archive/test/).\n",
    "- Inside, youâ€™ll typically see train, test, and validation folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798ccf6-02ee-427c-906f-00dd210a2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a balanced set of preprocessed AffectNet test images (excluding 'contempt') as calibration_data['image'] for quantization/evaluation\n",
    "\n",
    "from pathlib import Path\n",
    "import os, random, cv2, numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# Point to the test split in your local folder\n",
    "AFFECTNET_DIR = Path(os.getcwd()) / \"archive\" / \"test\"\n",
    "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "EXCLUDE_CLASS = \"contempt\"\n",
    "PER_CLASS_LIMIT = 100\n",
    "RNG_SEED = 42\n",
    "\n",
    "BGR_MEAN = np.array([91.4953, 103.8827, 131.0912], dtype=np.float32)\n",
    "\n",
    "def preprocess_caffe_bgr_from_pil(pil_img, size=(224, 224)):\n",
    "    rgb = np.asarray(pil_img.convert('RGB'))\n",
    "    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "    bgr = cv2.resize(bgr, size, interpolation=cv2.INTER_LINEAR).astype(np.float32)\n",
    "    bgr -= BGR_MEAN\n",
    "    x = np.transpose(bgr, (2, 0, 1))\n",
    "    return np.expand_dims(x, 0)\n",
    "\n",
    "# Build dict: class -> image paths\n",
    "class_to_paths = {}\n",
    "for emotion_folder in os.listdir(AFFECTNET_DIR):\n",
    "    folder_path = AFFECTNET_DIR / emotion_folder\n",
    "    if not folder_path.is_dir():\n",
    "        continue\n",
    "    if emotion_folder.lower() == EXCLUDE_CLASS.lower():\n",
    "        print(f\"[INFO] Skipping folder: {emotion_folder}\")\n",
    "        continue\n",
    "    imgs = [str(folder_path / f) for f in os.listdir(folder_path) if f.lower().endswith(IMG_EXTS)]\n",
    "    if imgs:\n",
    "        class_to_paths[emotion_folder] = imgs\n",
    "\n",
    "# Summary\n",
    "total_avail = sum(len(v) for v in class_to_paths.values())\n",
    "print(f\"Found {total_avail} images across {len(class_to_paths)} folders (excluding '{EXCLUDE_CLASS}').\")\n",
    "\n",
    "# Balanced selection\n",
    "rng = random.Random(RNG_SEED)\n",
    "selected_paths = []\n",
    "for cls, paths in class_to_paths.items():\n",
    "    rng.shuffle(paths)\n",
    "    take_count = min(PER_CLASS_LIMIT, len(paths))\n",
    "    selected_paths.extend(paths[:take_count])\n",
    "\n",
    "rng.shuffle(selected_paths)\n",
    "\n",
    "# Preprocess\n",
    "sample_inputs, bad = [], 0\n",
    "for i, img_path in enumerate(selected_paths):\n",
    "    try:\n",
    "        pil_img = Image.open(img_path)\n",
    "        arr = preprocess_caffe_bgr_from_pil(pil_img)\n",
    "        sample_inputs.append(arr)\n",
    "        if i < 8:\n",
    "            print(f\"Sample {i}: {os.path.basename(img_path)} shape={arr.shape}\")\n",
    "    except Exception as e:\n",
    "        bad += 1\n",
    "        print(f\"[Skip] {img_path}: {e}\")\n",
    "\n",
    "print(f\"Prepared {len(sample_inputs)} calibration samples. Skipped {bad} problematic images.\")\n",
    "\n",
    "# Validate\n",
    "expected_shape = (1, 3, 224, 224)\n",
    "if not all(arr.shape == expected_shape for arr in sample_inputs):\n",
    "    raise ValueError(\"Shape mismatch\")\n",
    "calibration_data = {\"image\": sample_inputs}\n",
    "print(f\"[OK] calibration_data['image'] has {len(calibration_data['image'])} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00ddbd-9733-44ff-998a-8f2a1c8010bf",
   "metadata": {},
   "source": [
    "Option B: Synthetic Calibration Samples (No External Data Needed)\n",
    "--------------------------\n",
    "This approach generates artificial images that mimic natural image statistics for post-training quantization calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87758f80-c47e-452a-96b7-fa7f8aa5becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Synthetic calibration samples (no external data needed)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# --------- Configuration ---------\n",
    "NUM_SAMPLES = 512\n",
    "IMG_SIZE = (224, 224)              # (width, height)\n",
    "INPUT_NAME = \"image\"\n",
    "RNG_SEED = 42\n",
    "\n",
    "# Caffe BGR mean\n",
    "BGR_MEAN = np.array([91.4953, 103.8827, 131.0912], dtype=np.float32)\n",
    "\n",
    "# Synthetic noise settings\n",
    "NOISE_STD = 45.0\n",
    "ADD_SHAPES = True\n",
    "NUM_SHAPES_PER_IMG = (1, 4)\n",
    "\n",
    "# --------- Preprocess (matches model pipeline) ---------\n",
    "def preprocess_caffe_bgr_from_pil(pil_img, size=(224, 224)):\n",
    "    rgb = np.asarray(pil_img.convert('RGB'))\n",
    "    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "    bgr = cv2.resize(bgr, size, interpolation=cv2.INTER_LINEAR).astype(np.float32)\n",
    "    bgr -= BGR_MEAN\n",
    "    x = np.transpose(bgr, (2, 0, 1))  # CHW\n",
    "    return np.expand_dims(x, 0)       # NCHW (1,3,H,W)\n",
    "\n",
    "# --------- Synthetic image generator ---------\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "def random_bgr_image(size):\n",
    "    h, w = size[1], size[0]\n",
    "    base = rng.normal(loc=BGR_MEAN, scale=NOISE_STD, size=(h, w, 3)).astype(np.float32)\n",
    "    base = np.clip(base, 0, 255)\n",
    "\n",
    "    if ADD_SHAPES:\n",
    "        img = base.copy().astype(np.uint8)\n",
    "        num_shapes = rng.integers(NUM_SHAPES_PER_IMG[0], NUM_SHAPES_PER_IMG[1] + 1)\n",
    "        for _ in range(num_shapes):\n",
    "            color = tuple(int(c) for c in rng.integers(0, 256, size=3))  # BGR\n",
    "            thickness = int(rng.integers(1, 4))\n",
    "            shape_type = int(rng.integers(0, 3))  # 0=circle, 1=rectangle, 2=line\n",
    "            if shape_type == 0:\n",
    "                center = (int(rng.integers(0, w)), int(rng.integers(0, h)))\n",
    "                radius = int(rng.integers(min(h, w) // 20, min(h, w) // 6))\n",
    "                cv2.circle(img, center, radius, color, thickness)\n",
    "            elif shape_type == 1:\n",
    "                p1 = (int(rng.integers(0, w)), int(rng.integers(0, h)))\n",
    "                p2 = (int(rng.integers(0, w)), int(rng.integers(0, h)))\n",
    "                cv2.rectangle(img, p1, p2, color, thickness)\n",
    "            else:\n",
    "                p1 = (int(rng.integers(0, w)), int(rng.integers(0, h)))\n",
    "                p2 = (int(rng.integers(0, w)), int(rng.integers(0, h)))\n",
    "                cv2.line(img, p1, p2, color, thickness)\n",
    "        base = img.astype(np.float32)\n",
    "\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(base.astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "    return pil_img\n",
    "\n",
    "# --------- Build synthetic calibration dataset ---------\n",
    "sample_inputs = []\n",
    "for i in range(NUM_SAMPLES):\n",
    "    pil_img = random_bgr_image(IMG_SIZE)\n",
    "    arr = preprocess_caffe_bgr_from_pil(pil_img, size=IMG_SIZE)\n",
    "    sample_inputs.append(arr)\n",
    "    if i < 8:\n",
    "        print(f\"[Synthetic] Sample {i}: shape={arr.shape} dtype={arr.dtype}\")\n",
    "\n",
    "# --------- Validate ---------\n",
    "expected_shape = (1, 3, IMG_SIZE[1], IMG_SIZE[0])  # (N, C, H, W)\n",
    "all_shapes_ok = all(arr.shape == expected_shape for arr in sample_inputs)\n",
    "all_dtypes_ok = all(arr.dtype == np.float32 for arr in sample_inputs)\n",
    "print(\"All shapes correct?:\", all_shapes_ok)\n",
    "print(\"All dtypes float32?:\", all_dtypes_ok)\n",
    "if not all_shapes_ok or not all_dtypes_ok or len(sample_inputs) == 0:\n",
    "    raise ValueError(\"Calibration samples invalid: shape/dtype mismatch or empty dataset.\")\n",
    "\n",
    "# --------- Package for quantization ---------\n",
    "calibration_data = {INPUT_NAME: sample_inputs}\n",
    "print(f\"[OK] calibration_data['{INPUT_NAME}'] has {len(calibration_data[INPUT_NAME])} samples (each {expected_shape}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f1ba0-4fa9-4ac1-b846-6dcedda82179",
   "metadata": {},
   "source": [
    "Quantize ONNX model using AI Hub (INT8 weights & activations).\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2010203-1cc2-4271-893a-644b1cceebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the compiled ResNet50 emotion model to INT8 using representative calibration data for efficient on-device inference.\n",
    "\n",
    "import qai_hub as hub\n",
    "\n",
    "print(\"[INFO] Submitting quantization job (INT8 weights & activations)...\")\n",
    "quantize_job = hub.submit_quantize_job(\n",
    "    model=static_model,\n",
    "    calibration_data=calibration_data,  # List of per-sample arrays (each (1,3,H,W))\n",
    "    weights_dtype=hub.QuantizeDtype.INT8,\n",
    "    activations_dtype=hub.QuantizeDtype.INT8,\n",
    ")\n",
    "\n",
    "quantize_job.wait()\n",
    "quantized_model = quantize_job.get_target_model()\n",
    "print(\"[INFO] Quantization complete. Model saved at:\", quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dd708-d5af-42d4-8ef1-d98a997ae08b",
   "metadata": {},
   "source": [
    "Compile quantized model to TFLite for RB3 deployment using AI Hub.\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710f61c-e47d-41e1-b774-5f1063a838d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the INT8-quantized emotion model to TFLite and download it for deployment on the RB3 Gen 2 device\n",
    "\n",
    "import qai_hub as hub\n",
    "\n",
    "print(\"[INFO] Submitting TFLite compile job...\")\n",
    "compile_job = hub.submit_compile_job(\n",
    "    model=quantized_model,\n",
    "    device=hub.Device(\"Dragonwing RB3 Gen 2 Vision Kit\"),\n",
    "    name=\"EmotionModel_TFLite\",\n",
    "    input_specs=None,\n",
    "    options=\"--target_runtime tflite\",\n",
    ")\n",
    "\n",
    "compile_job.wait()\n",
    "tflite_model = compile_job.get_target_model()\n",
    "tflite_model.download(\"emotion_quant_model.tflite\")\n",
    "\n",
    "print(\"[INFO] TFLite model downloaded successfully for RB3 deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08039d9-c091-43e7-9bc8-d5c0b847d093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
